'''
this is an automated scraper that reads the missing_dragons.json file 
generated by the check_dragon_list.py

it then scrapes the dragon's data based on the name provided in 
missing_dragons.json file

NOTE:- this method might fail due to inconsistencies in dragon
naming conventions, in case a new dragon's data gets missed 
then it can scraped using manual_scrape.py

'''


import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, Any, List
from urllib.parse import urljoin
import datetime

class DragonScraper:
    def __init__(self):
        self.base_url = "https://dvc.wiki.gg"
        self.dragons = {}
        self.session = self._create_session()

    def _create_session(self) -> requests.Session:
        """Create a session with headers"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        return session

    def scrape_new_dragons(self, new_dragon_keys: List[str]):
        """Scrape only the new dragons from the provided list"""
        total_dragons = len(new_dragon_keys)
        print(f"Found {total_dragons} new dragons to scrape")

        for index, dragon_key in enumerate(new_dragon_keys, 1):
            print(f"Processing {dragon_key} ({index}/{total_dragons})")
            
            # Construct URL for the dragon
            url = f"{self.base_url}/wiki/{dragon_key.capitalize()}"
            
            # Scrape the dragon
            dragon_data = self.scrape_dragon(url)
            if dragon_data:
                self.dragons[dragon_key.lower()] = dragon_data
                print(f"Successfully scraped {dragon_key}")
            else:
                print(f"Failed to scrape {dragon_key}")
            
            # Save progress after each dragon
            self.save_progress()
            # time.sleep(1)  # Small delay between requests
        
        print("\nScraping completed!")
        print(f"Successfully scraped: {len(self.dragons)} dragons")
        return self.dragons

    def scrape_dragon(self, dragon_key: str) -> Dict[str, Any]:
        """Scrape a single dragon's data"""
        url = f"{self.base_url}/wiki/{dragon_key}"
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find the tabber container
                tabber = soup.find('div', class_='tabber')
                forms = []
                
                if tabber:
                    # Find all tab elements
                    tabs = tabber.find_all('a', class_='tabber__tab')
                    for tab in tabs:
                        form_name = tab.text.strip().lower()
                        # Skip any empty tabs
                        if form_name:
                            forms.append(form_name)
                
                # If no forms found, at least add "default"
                if not forms:
                    forms = ["default"]

                # Get the rest of the dragon info as before
                infobox = soup.find('aside', class_='portable-infobox')
                if not infobox:
                    return {}

                # Format dragon name from key
                formatted_name = " ".join(word.capitalize() for word in dragon_key.replace("_", " ").split())

                dragon_info = {
                    "name": formatted_name,
                    "egg_description": self._get_infobox_value(infobox, 'EggDescription'),
                    "elements": self._get_elements(infobox),
                    "rarity": self._get_infobox_value(infobox, 'Rarity'),
                    "region": self._get_infobox_value(infobox, 'Region'),
                    "dragon_type": self._get_infobox_value(infobox, 'DragonType'),
                    "body_type": self._get_infobox_value(infobox, 'BodyType'),
                    "food": self._get_infobox_value(infobox, 'Food'),
                    "tradeable": self._get_tradeable(infobox),
                    "breeding_tier": self._get_breeding_tier(infobox),
                    "forms": forms,  # Add the forms list
                    "profile_img_url": self._get_profile_image(soup),
                    "profile_url": url,
                    "egg_img_url": self._get_egg_image(soup)
                }

                return dragon_info
        except Exception as e:
            print(f"Error scraping dragon {dragon_key}: {e}")
            return {}

    def _get_infobox_value(self, infobox, data_source: str) -> str:
        """Helper method to get infobox values"""
        element = infobox.find('div', {'data-source': data_source})
        if element:
            value_div = element.find('div', class_='pi-data-value')
            if value_div:
                return value_div.text.strip()
        return ""

    def _get_elements(self, infobox) -> List[str]:
        """Extract elements from infobox"""
        elements_div = infobox.find('div', {'data-source': 'Elements'})
        elements = []
        if elements_div:
            for img in elements_div.find_all('img'):
                element = img.get('alt', '').lower().replace(' element icon.png', '')
                if element:
                    elements.append(element)
        return elements

    def _parse_breeding_tier(self, tier_text: str) -> int:
        """Parse breeding tier from text"""
        if tier_text:
            try:
                return int(''.join(filter(str.isdigit, tier_text)))
            except ValueError:
                pass
        return 0

    def save_progress(self):
        """Save dragon data to JSON"""
        try:
            # Load existing dragon list
            with open('dragon/updated_dragon_list.json', 'r') as f:
                existing_data = json.load(f)
        except FileNotFoundError:
            existing_data = {"dragon": [{}, 0, ""]}

        # Convert all keys to lowercase and sort
        sorted_dragons = {}
        for key, value in self.dragons.items():
            sorted_dragons[key.lower()] = value

        # Sort the dragon data keys
        sorted_dragons = dict(sorted(sorted_dragons.items()))
        
        # Update with sorted new dragons
        existing_data["dragon"][0].update(sorted_dragons)
        
        # Sort the entire dictionary
        existing_data["dragon"][0] = dict(sorted(existing_data["dragon"][0].items()))
        
        existing_data["dragon"][1] = len(existing_data["dragon"][0])
        existing_data["dragon"][2] = str(datetime.datetime.now())

        # Save updated list
        with open('dragon/updated_dragon_list.json', 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, indent=2, ensure_ascii=False)

def main():
    # Load the list of new dragons
    try:
        with open('missing_dragons.json', 'r') as f:
            new_dragon_keys = json.load(f)
    except FileNotFoundError:
        print("missing_dragons.json not found. Please run check_dragon_list.py first.")
        return

    # Initialize and run scraper
    scraper = DragonScraper()
    try:
        new_dragons = scraper.scrape_new_dragons(new_dragon_keys)
        print(f"Successfully scraped {len(new_dragons)} new dragons")
    except Exception as e:
        print(f"An error occurred: {str(e)}")
    finally:
        scraper.save_progress()

if __name__ == "__main__":
    main() 